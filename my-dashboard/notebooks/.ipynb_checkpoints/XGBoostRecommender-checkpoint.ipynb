{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a085483-a6e1-4bfa-acce-7a34cd0cf49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "RISK_FREE = 0.043  # 4.3% annual\n",
    "\n",
    "def portfolio_return(weights, mu):\n",
    "    \"\"\"Annualised expected return.\"\"\"\n",
    "    return np.dot(weights, mu)\n",
    "\n",
    "def portfolio_vol(weights, cov):\n",
    "    \"\"\"Annualised volatility (std dev).\"\"\"\n",
    "    return np.sqrt(weights @ cov @ weights)\n",
    "\n",
    "def sharpe_ratio(weights, mu, cov):\n",
    "    \"\"\"Sharpe ratio: (E[R] - Rf) / σ.\"\"\"\n",
    "    ret = portfolio_return(weights, mu)\n",
    "    vol = portfolio_vol(weights, cov)\n",
    "    return (ret - RISK_FREE) / vol\n",
    "\n",
    "def cvar_calc(weights, mu, cov, alpha=0.95):\n",
    "    \"\"\"\n",
    "    Approximate CVaR under Normal assumption:\n",
    "      CVaR = -(mean - σ * φ(Ζα)/(1-α))\n",
    "    where Ζα = norm.ppf(α)\n",
    "    \"\"\"\n",
    "    ret = portfolio_return(weights, mu)\n",
    "    vol = portfolio_vol(weights, cov)\n",
    "    z = st.norm.ppf(alpha)\n",
    "    pdf = st.norm.pdf(z)\n",
    "    cvar = -(ret - vol * pdf / (1 - alpha))\n",
    "    return cvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20099c8a-4233-4c92-8aea-32e26bd41239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxopt import matrix, solvers\n",
    "\n",
    "def mean_variance_opt(mu, cov, target_return):\n",
    "    \"\"\"\n",
    "    Solve:\n",
    "      minimize 0.5 w^T Σ w\n",
    "      s.t.     μ^T w >= target_return\n",
    "               sum(w) = 1\n",
    "               w >= 0\n",
    "    Returns optimal weights as a NumPy array.\n",
    "    \"\"\"\n",
    "    n = len(mu)\n",
    "    P = matrix(cov * 2)                 # 2Σ\n",
    "    q = matrix(np.zeros(n))             # zero vector\n",
    "\n",
    "    # Constraints Gx <= h  <=>  -Ix <= 0  (w >= 0)\n",
    "    G = matrix(-np.eye(n))\n",
    "    h = matrix(np.zeros(n))\n",
    "\n",
    "    # Constraints Ax = b for equalities and inequalities combined\n",
    "    # We'll stack [1^T; μ^T] for equalities, then the inequality μ^T w >= target => -μ^T w <= -target\n",
    "    A = matrix(np.vstack([np.ones((1, n)), mu.reshape(1, n)]))\n",
    "    b = matrix([1.0, target_return])\n",
    "\n",
    "    # To handle μ^T w >= target, we include it as an inequality by appending to G,h:\n",
    "    # -μ^T w <= -target → G = [ -I; -μ ]  and  h = [ 0; -target ]\n",
    "    G = matrix(np.vstack([np.eye(n)*-1, mu.reshape(1,n)*-1]))\n",
    "    h = matrix(np.hstack([np.zeros(n), -target_return]))\n",
    "\n",
    "    sol = solvers.qp(P, q, G, h, A, b)\n",
    "    w = np.array(sol['x']).flatten()\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23bc4af7-5884-40bc-822d-1363e61bea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, requests, time, itertools, math\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "API_KEY = os.getenv(\"ALPHA_VANTAGE_KEY\")\n",
    "BASE    = \"https://www.alphavantage.co/query\"\n",
    "\n",
    "# 25-ticker “mini-universe” (20 blue-chip stocks + 5 broad ETFs)\n",
    "UNIVERSE = [\n",
    "    # 20 Stocks\n",
    "    \"AAPL\",\"MSFT\",\"AMZN\",\"GOOG\",\"TSLA\",\n",
    "    \"NVDA\",\"BRK.B\",\"JNJ\",\"V\",\"JPM\",\n",
    "    \"UNH\",\"HD\",\"PG\",\"MA\",\"XOM\",\n",
    "    \"CVX\",\"PFE\",\"MRK\",\"KO\",\"WMT\",\n",
    "\n",
    "    # 5 ETFs\n",
    "    \"SPY\",\"QQQ\",\"VTI\",\"IWM\",\"AGG\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79bfb73-3f3b-4a0b-9329-22b1bee7a258",
   "metadata": {},
   "source": [
    " ## Fetch weekly‐adjusted prices (cache locally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0940157b-9ed5-42c0-a06d-2ae75c810185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "CACHE = Path(\"av_cache\"); CACHE.mkdir(exist_ok=True)\n",
    "\n",
    "def av_get(function, symbol):\n",
    "    key = f\"{function}_{symbol}.json\"\n",
    "    fp = CACHE / key\n",
    "    if fp.exists():\n",
    "        return json.loads(fp.read_text())\n",
    "    url = f\"{BASE}?function={function}&symbol={symbol}&apikey={API_KEY}\"\n",
    "    r = requests.get(url); r.raise_for_status()\n",
    "    fp.write_text(r.text)\n",
    "    time.sleep(12)                       # AV free tier = 5 calls/min\n",
    "    return r.json()\n",
    "\n",
    "def weekly_series(sym):\n",
    "    js = av_get(\"TIME_SERIES_WEEKLY_ADJUSTED\", sym)\n",
    "    df = (pd.DataFrame(js[\"Weekly Adjusted Time Series\"])\n",
    "            .T.astype(float)[[\"4. close\"]].rename(columns={\"4. close\":\"close\"}))\n",
    "    return df.sort_index()\n",
    "\n",
    "def overview(sym):\n",
    "    return av_get(\"OVERVIEW\", sym)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433c5364-e784-43e3-bb5a-c881b60542f8",
   "metadata": {},
   "source": [
    "## Unified Price Table and Momentum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c22f5-cb25-460c-9495-9ddeb5344b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_dfs = {}\n",
    "for sym in UNIVERSE:\n",
    "    try:\n",
    "        price_dfs[sym] = weekly_series(sym)[\"close\"]\n",
    "    except Exception as e:\n",
    "        print(\"skip\", sym, e)\n",
    "\n",
    "prices = pd.concat(price_dfs, axis=1).dropna(how=\"all\")\n",
    "rets   = prices.pct_change().dropna()\n",
    "μ      = rets.mean() * 52                        # annualised mean\n",
    "Σ      = rets.cov()  * 52                        # annualised cov\n",
    "\n",
    "# Momentum 6m / 12m\n",
    "mom_6  = prices.pct_change(26).iloc[-1]\n",
    "mom_12 = prices.pct_change(52).iloc[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b148385-d1a4-4f32-8fa3-6fa6d1a501b1",
   "metadata": {},
   "source": [
    "## Fundamental Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83129ded-0d3a-4766-9400-e6f55c8ed374",
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_rows = []\n",
    "for sym in UNIVERSE:\n",
    "    try:\n",
    "        o = overview(sym)\n",
    "        fund_rows.append({\n",
    "          \"ticker\": sym,\n",
    "          \"beta\":         float(o.get(\"Beta\", np.nan)),\n",
    "          \"divYield\":     float(o.get(\"DividendYield\", np.nan)),\n",
    "          \"logCap\":       math.log(float(o.get(\"MarketCapitalization\", 1))),\n",
    "          \"sector\":       o.get(\"Sector\", \"N/A\")\n",
    "        })\n",
    "    except: pass\n",
    "\n",
    "fund = pd.DataFrame(fund_rows).set_index(\"ticker\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b1a48e-332b-4e0b-b123-b3ed5a69ea23",
   "metadata": {},
   "source": [
    "## Building Training Data via Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03aceae-8b5a-47c4-b640-077f1f474ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def simulate_portfolios(num=2000, size=20):\n",
    "    rng = np.random.default_rng(0)\n",
    "    sims = []\n",
    "    for _ in range(num):\n",
    "        basket = rng.choice(UNIVERSE, size=size, replace=False)\n",
    "        w0     = rng.dirichlet(np.ones(size))\n",
    "        sims.append(dict(zip(basket, w0)))\n",
    "    return sims\n",
    "\n",
    "def label_asset(port, asset):\n",
    "    basket = list(port.keys()) + [asset]\n",
    "    weights= np.append(list(port.values()), 0)      # start with 0% weight\n",
    "    w_opt  = mean_variance_opt(μ[basket], Σ.loc[basket, basket], 0.13) # target 13%\n",
    "    base   = sharpe_ratio(weights[:-1], μ[basket[:-1]], Σ.loc[basket[:-1], basket[:-1]])\n",
    "    new    = sharpe_ratio(w_opt, μ[basket], Σ.loc[basket, basket])\n",
    "    return int(new > base+1e-4)\n",
    "\n",
    "# --- build dataset ---\n",
    "records = []\n",
    "TARGETS = [0.08, 0.13, 0.18, 0.20, 0.25]  # 8%, 13%, 18%, 20%, 25%\n",
    "\n",
    "for port in simulate_portfolios():\n",
    "    held = set(port)\n",
    "    base_tickers = list(port.keys())\n",
    "    base_w = np.array(list(port.values()))\n",
    "\n",
    "    # compute base Sharpe/CVaR once per portfolio\n",
    "    base_sh = sharpe_ratio(\n",
    "        base_w,\n",
    "        μ[base_tickers].values,\n",
    "        Σ.loc[base_tickers, base_tickers].values\n",
    "    )\n",
    "    base_cv = cvar_calc(\n",
    "        base_w,\n",
    "        μ[base_tickers].values,\n",
    "        Σ.loc[base_tickers, base_tickers].values\n",
    "    )\n",
    "\n",
    "    for target in TARGETS:\n",
    "        # compute the global optimum for this target\n",
    "        # note: mean_variance_opt solves for w* s.t. sum(w*)=1, w*>=0, μ·w*>=target\n",
    "        w_star = mean_variance_opt(\n",
    "            mu=μ.values,\n",
    "            cov=Σ.values,\n",
    "            target_return=target\n",
    "        )\n",
    "\n",
    "        # map w_star back to ticker list\n",
    "        # assume UNIVERSAL ordering same as μ.index\n",
    "        # so w_star[i] corresponds to μ.index[i]\n",
    "        # heldIdx = [i for i,sym in enumerate(μ.index) if sym in base_tickers]\n",
    "\n",
    "        for cand in UNIVERSE:\n",
    "            if cand in held:\n",
    "                continue\n",
    "\n",
    "            # marginal tilt of ε=1%\n",
    "            eps = 0.01\n",
    "            # take eps from the largest weight in w_star\n",
    "            donor = np.argmax(w_star)\n",
    "            w_pert = w_star.copy()\n",
    "            w_pert[donor] = max(0, w_pert[donor] - eps)\n",
    "            idx = list(μ.index).index(cand)\n",
    "            w_pert[idx] += eps\n",
    "\n",
    "            # compute perturbed metrics\n",
    "            pert_sh = sharpe_ratio(\n",
    "                w_pert,\n",
    "                μ.values,\n",
    "                Σ.values\n",
    "            )\n",
    "            pert_cv = cvar_calc(\n",
    "                w_pert,\n",
    "                μ.values,\n",
    "                Σ.values\n",
    "            )\n",
    "\n",
    "            rec = {\n",
    "                \"ticker\":      cand,\n",
    "                \"targetReturn\": target,\n",
    "                \"deltaSharpe\":  pert_sh - base_sh,\n",
    "                \"deltaCvar\":    base_cv - pert_cv,\n",
    "                \"mom6\":         mom_6[cand],\n",
    "                \"mom12\":        mom_12[cand],\n",
    "                **fund.loc[cand].to_dict(),\n",
    "                \"label\":        label_asset(port, cand, target)\n",
    "            }\n",
    "            records.append(rec)\n",
    "\n",
    "df = pd.DataFrame(records).dropna()\n",
    "# include 'targetReturn' among features\n",
    "feature_cols = [\n",
    "    \"deltaSharpe\",\"deltaCvar\",\"mom6\",\"mom12\",\n",
    "    \"beta\",\"divYield\",\"logCap\",\"targetReturn\"\n",
    "]\n",
    "X = df[feature_cols]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# train/val/test split as before\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2814d754-c069-42b7-9dc6-4dccb452e63e",
   "metadata": {},
   "source": [
    "## Run Grid Search with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdc50a9-3145-447e-8c3c-b474f821584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, ParameterGrid\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# handle class imbalance\n",
    "pos_weight = (len(y_train) - y_train.sum()) / y_train.sum()\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\":        [3, 4, 5],\n",
    "    \"learning_rate\":    [0.03, 0.06, 0.1],\n",
    "    \"n_estimators\":     [400, 700, 1000],   # will be cut by early stopping\n",
    "    \"subsample\":        [0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.8, 1.0],\n",
    "    \"min_child_weight\": [1, 3, 5],\n",
    "    \"gamma\":            [0, 1]\n",
    "}\n",
    "\n",
    "best_auc, best_params, best_model = -1, None, None\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    clf = xgb.XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        tree_method=\"hist\",\n",
    "        eval_metric=\"auc\",\n",
    "        random_state=42,\n",
    "        scale_pos_weight=float(pos_weight),\n",
    "        **params\n",
    "    )\n",
    "\n",
    "    clf.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False,\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "\n",
    "    auc = roc_auc_score(y_val, clf.predict_proba(X_val)[:, 1])\n",
    "    if auc > best_auc:\n",
    "        best_auc, best_params, best_model = auc, params, clf\n",
    "        print(f\"New best AUC {auc:.3f} with {params}  (iters={clf.best_iteration_})\")\n",
    "\n",
    "print(\"Best validation AUC:\", best_auc)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c133c27b-8d9e-4a72-96a3-0eae3a455320",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3239223c-edef-4b56-b915-0654d9d285cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, classification_report\n",
    "\n",
    "proba_test = best_model.predict_proba(X_test)[:,1]\n",
    "auc_test   = roc_auc_score(y_test, proba_test)\n",
    "ap_test    = average_precision_score(y_test, proba_test)\n",
    "print(f\"Test AUC: {auc_test:.3f} | PR-AUC: {ap_test:.3f}\")\n",
    "\n",
    "# choose a threshold using validation (Youden J)\n",
    "proba_val = best_model.predict_proba(X_val)[:,1]\n",
    "thr = np.quantile(proba_val, 0.7)  # or search argmax(tpr - fpr)\n",
    "pred_test = (proba_test >= thr).astype(int)\n",
    "print(classification_report(y_test, pred_test, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1b69fa-91cc-4e95-8dc8-8f8de2d9586c",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc77c63-aa06-4ffa-a7da-965358ed54dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib, pathlib\n",
    "\n",
    "best_model.save_model(\"../src/lib/recommend_model.json\")\n",
    "\n",
    "# (If you’re keeping a Python-only workflow, you could also do:)\n",
    "# import joblib\n",
    "# joblib.dump(best_model, \"../src/lib/models/recommend_model.joblib\")\n",
    "\n",
    "print(\"Saved to ../lib/models/recommend_model.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
